import art
from textwrap import dedent
from litellm import acompletion
from rich import print
import litellm
from dotenv import load_dotenv
import json
from dataclasses import asdict
from typing import Optional

from litellm.caching.caching import LiteLLMCacheType, Cache
from lecture_search_tools import search_lectures, read_lecture, get_speaker_stats, get_session_list
from langchain_core.utils.function_calling import convert_to_openai_tool
from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt
from project_types import LectureScenario
import weave
from art.utils.litellm import convert_litellm_choice_to_openai

litellm.cache = Cache(type=LiteLLMCacheType.DISK)


load_dotenv()

MAX_TURNS = 10

weave.init(project_name="lecture-rl")


class CorrectnessJudgeResponse(BaseModel):
    reasoning: str = Field(description="Explanation of the reasoning process.")
    accept: bool = Field(description="Whether the AI answer should be accepted.")


@retry(stop=stop_after_attempt(3))
async def judge_correctness(
    scenario: LectureScenario, answer: str
) -> CorrectnessJudgeResponse:
    system_prompt = dedent(
        """
        You are given a question, the reference answer (labelled **Reference answer**), and an answer generated by an AI assistant (labelled **AI answer**).

        Your task is to decide whether the AI answer is correct and should be accepted. You should accept the answer if it contains the relevant information from the reference answer. You should not accept the answer if it is missing information relevant to the question, or if it contradicts the reference answer.
        """
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {
            "role": "user",
            "content": (
                f"Question: {scenario.question}\n"
                f"Reference answer: {scenario.answer}\n"
                f"AI answer: {answer}"
            ),
        },
    ]

    response = await acompletion(
        model="openai/gpt-4.1",
        messages=messages,
        caching=True,
        response_format=CorrectnessJudgeResponse,
    )

    first_choice = response.choices[0]  # type: ignore[attr-defined]
    raw_content = first_choice.message.content or "{}"  # type: ignore[attr-defined]

    try:
        return CorrectnessJudgeResponse.model_validate_json(raw_content)
    except Exception as e:
        # If parsing fails, fall back to 'accept': False
        return CorrectnessJudgeResponse(
            reasoning=f"Parse error: {e}\nRaw: {raw_content}", accept=False
        )


class FinalAnswer(BaseModel):
    answer: str
    source_ids: list[str]


class ProjectTrajectory(art.Trajectory):
    final_answer: FinalAnswer | None = None


@weave.op()
async def run_agent(model: art.Model, scenario: LectureScenario) -> ProjectTrajectory:
    traj = ProjectTrajectory(
        reward=0.0,
        messages_and_choices=[],
    )
    
    system_prompt = dedent(
        f"""
        You are a lecture search agent. You are given a user query and a list of tools you can use to search lecture transcripts. Use the tools to search the lecture database and find the answer to the user's query. You may take up to {MAX_TURNS} turns to find the answer, so if your first search doesn't find the answer, you can try with different keywords.
        
        The database contains transcripts from reinforcement learning lectures and office hours sessions.
        Session type: {scenario.session_type}
        Session number: {scenario.session_number}
        """
    )
    
    traj.messages_and_choices = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": scenario.question},
    ]
    
    def search_lecture_database(keywords: list[str]) -> list[dict]:
        """Search the lecture database for entries matching the given keywords and return
        a list of dictionaries so the LLM can easily consume them."""
        results = search_lectures(
            keywords=keywords,
            session_type=scenario.session_type,
            session_number=scenario.session_number,
            max_results=10
        )
        # Convert each SearchResult dataclass instance to a plain dict
        return [asdict(result) for result in results]
    
    def return_final_answer(
        answer: str, reference_entry_ids: list[str]
    ) -> FinalAnswer:
        """Return the final answer and the entry IDs of the lecture entries that were used to generate the answer."""
        return FinalAnswer(answer=answer, source_ids=reference_entry_ids)
    
    tools = [search_lecture_database, read_lecture, return_final_answer]
    tools_by_name = {t.__name__: t for t in tools}
    traj.tools = [convert_to_openai_tool(t) for t in tools]  # type: ignore[attr-defined]
    
    if model.trainable:
        litellm_model_name = f"hosted_vllm/{model.name}"
    else:
        litellm_model_name = model.name
    
    for turn in range(MAX_TURNS):
        response = await acompletion(
            model=litellm_model_name,
            base_url=model.inference_base_url,
            api_key=model.inference_api_key,
            temperature=1,
            messages=traj.messages(),
            # caching=not model.trainable,
            caching=False,
            tools=traj.tools,
        )
        
        response_message = response.choices[0].message  # type: ignore[attr-defined]
        traj.messages_and_choices.append(
            convert_litellm_choice_to_openai(response.choices[0])  # type: ignore[attr-defined]
        )  # type: ignore[attr-defined]
        
        if response_message.content or not response_message.tool_calls:
            # We always want tool calls. If they're missing, the model isn't
            # behaving how we want and we should just return the trajectory.
            return traj
        
        try:
            for tool_call in response_message.tool_calls:
                tool_name: str = tool_call.function.name  # type: ignore
                if tool_name in tools_by_name:
                    tool_args = json.loads(tool_call.function.arguments)
                    tool_to_call = tools_by_name[tool_name]
                    result = tool_to_call(**tool_args)
                    traj.messages_and_choices.append(
                        {
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "name": tool_name,
                            "content": str(result),
                        }  # type: ignore[attr-defined]
                    )
                    
                    if tool_name == "return_final_answer":
                        traj.final_answer = result
                        return traj
        except Exception as e:
            print(f"Error parsing tool calls: {e}")
            return traj
    
    return traj


@weave.op()
async def run_agent_and_score(
    model: art.Model, scenario: LectureScenario
) -> ProjectTrajectory:
    traj = await run_agent(model, scenario)
    if traj.final_answer is None:
        traj.reward = 0.0
        return traj
    correctness_judge_response = await judge_correctness(
        scenario, traj.final_answer.answer
    )
    traj.reward = float(correctness_judge_response.accept)
    return traj


if __name__ == "__main__":
    import asyncio
    from load_scenarios import load_scenarios
    
    scenario = load_scenarios(limit=1)[0]
    model = art.Model(name="openai/gpt-4.1", project="lecture-rl")
    answer = asyncio.run(run_agent_and_score(model, scenario))
    print(answer)





